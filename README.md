# Explainable-AI---Lime-Interpretable-Model
XAI - By using the LIME Model, we can explain the predictions of any classifier or regressor model in faithful way. Lime is examples of techniques for training interpretable machine learning (ML) models, explaining ML models, and debugging ML models for accuracy.
In these project, we used the online dataset of images which consist upto 5000 images of three different animal mainly Cat, Dog and Panda.
Firstly, We used the CNN model which is kind of network architecture for deep learning algorithms and is specifically used for image recognition and tasks that involve the processing of pixel data
Basic Idea of Model : 
  1. Data collection
  2. Data processing
  3. Building Model 
  4. Prediction
After getting the predictions for the model, then we looking forward to our goal.
![image](https://user-images.githubusercontent.com/79745221/209521444-b492bd68-a3fa-4d01-87ef-51508d7fef2b.png)

GOAL : 
 Explain the why our model give the specific prediction.
 For Example : Suppose our model predict "It is dog" then our main goal is to Explain these result? means, On which bases our model predict, "It is dog!"...
  
  5. Used Lime Explainer to Explain the result.
  ![image](https://user-images.githubusercontent.com/79745221/209521236-9c937350-c649-41c3-8995-4fb2967b589a.png)
